# ATLAS Performance Analysis Report

## Data Pipeline

### Current Bottlenecks

| Component | Issue | Impact | Recommendation |
|-----------|-------|--------|----------------|
| `CrystalGraphBuilder.structure_to_graph()` | Per-structure neighbor search via `get_all_neighbors()` | ~15ms per structure â†’ ~19min for 76k | âœ… Already parallelized via `ProcessPoolExecutor` |
| `CrystalPropertyDataset.prepare()` | Full graph rebuild on each call without `.pt` cache | Minutes on large datasets | âœ… Has disk cache via `torch.save()` |
| `JARVISClient.load_dft_3d()` | ~500MB FigShare download on first use | One-time cost | âœ… Has resume support |
| Worker count | `n_workers=1` hardcoded (Windows spawn issue) | No parallelism on Windows | ðŸŸ¡ Consider `loky` backend for Windows |
| 3-body index computation | O(nÂ²) per atom for triplet enumeration | Quadratic growth with `max_neighbors` | ðŸŸ¡ Cap at `max_neighbors` already helps |

### Graph Construction Cost Estimate

| Dataset Size | Sequential (1 worker) | Parallel (4 workers, Linux) |
|-------------|----------------------|----------------------------|
| 1,000 | ~15s | ~5s |
| 10,000 | ~150s | ~45s |
| 76,000 (full JARVIS) | ~19min | ~5min |

> **Note**: Disk cache prevents repeated construction. First run is slow, subsequent runs load from `.pt` in seconds.

---

## Training Efficiency

| Feature | Status | Notes |
|---------|--------|-------|
| AMP (Automatic Mixed Precision) | âœ… Implemented | Correctly disabled on CPU |
| Gradient Clipping | âœ… `max_norm=1.0` | Fixed threshold, consider adaptive |
| Gradient Accumulation | âš ï¸ `theory_tuning.py` defines `acc-steps` | Not implemented in `Trainer.train_epoch()` |
| DataLoader `pin_memory` | âš ï¸ Not set | Add `pin_memory=True` for GPU training |
| DataLoader `num_workers` | âš ï¸ Fixed at 1 (Windows) | Benchmark with 2-4 on Linux |
| Learning Rate Scheduling | âœ… ReduceLROnPlateau + others | Works correctly |
| Top-K Checkpointing | âš ï¸ Only saves best + final | Consider top-3 for ensemble |

---

## Memory Usage

| Risk | Module | Details | Mitigation |
|------|--------|---------|------------|
| ðŸŸ¡ Medium | `CrystalPropertyDataset` | All PyG `Data` objects held in memory | Fine for JARVIS (~76k Ã— ~2KB = ~150MB) |
| ðŸŸ¡ Medium | `EnsembleUQ` (5 models) | 5Ã— model parameters | Use shared embeddings or distillation |
| ðŸŸ¡ Medium | `M3GNet` triplet tensors | `O(N Ã— max_neighborsÂ²)` per graph | Capped at `max_neighbors=12` â†’ max 144 triplets/atom |
| ðŸŸ¢ Low | `EquivariantGNN` with `e3nn` | Dense tensor products | Typical: 10-50MB per model |

---

## Recommendations (Priority Ordered)

1. **P1**: Add `pin_memory=True` to DataLoader creation for GPU workflows
2. **P1**: Implement gradient accumulation in `Trainer.train_epoch()` to match `theory_tuning.py` profiles
3. **P2**: Benchmark `num_workers=2-4` on Linux CI, keep `=1` as Windows fallback
4. **P2**: Add `--prefetch-factor` option for DataLoader
5. **P3**: Consider lazy loading for very large datasets (>100k)
